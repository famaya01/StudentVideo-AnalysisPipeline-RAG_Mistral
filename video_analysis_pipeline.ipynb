{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install dependencies\n",
    "\n",
    "In terminal run:\n",
    "- python -m venv venv\n",
    "-  .\\venv\\Scripts\\activate \n",
    "- pip install -r requirements.txt\n",
    "- pip install -U openai-whisper\n",
    "- pip install llama-index-embeddings-huggingface\n",
    "-  pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
    "- llama-index-llms-llama-cpp\n",
    "- add ffmeg to scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\famaya\\Desktop\\quickstart\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "import os\n",
    "import whisper\n",
    "from docx import Document\n",
    "import llama_index\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.core import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import docx\n",
    "from docx import Document\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Extraction and Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "def extract_audio(video_path, output_folder):\n",
    "    \"\"\"\n",
    "    Extracts audio from a video file and saves it as an MP3 file in the specified output folder.\n",
    "\n",
    "    Args:\n",
    "    video_path (str): Path to the input video file.\n",
    "    output_folder (str): Path to the folder where the output audio file will be saved.\n",
    "    \"\"\"\n",
    "   \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "  \n",
    "    video_basename = os.path.basename(video_path)\n",
    "    audio_filename = os.path.splitext(video_basename)[0] + '.mp3'\n",
    "    audio_path = os.path.join(output_folder, audio_filename)\n",
    "\n",
    "   \n",
    "    (\n",
    "        ffmpeg\n",
    "        .input(video_path)\n",
    "        .output(audio_path)\n",
    "        .run()\n",
    "    )\n",
    "    print(f\"Audio extraction complete. The file has been saved to: {audio_path}\")\n",
    "    return audio_path\n",
    "\n",
    "def transcribe_audio(audio_path, output_folder):\n",
    "    \"\"\"\n",
    "    transcribes audio file with whisper and saves it as a docx in the specified output folder\n",
    "    \n",
    "    Args:\n",
    "    audio_path (str): Path to the input video file.\n",
    "    output_folder (str): Path to the folder where the output docx file will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    result = model.transcribe(audio_path)  \n",
    "\n",
    "   \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "\n",
    "    audio_basename = os.path.basename(audio_path)\n",
    "    docx_filename = os.path.splitext(audio_basename)[0] + '.docx'  \n",
    "    output_file_path = os.path.join(output_folder, docx_filename)\n",
    "    \n",
    "\n",
    "    doc = Document()\n",
    "    doc.add_paragraph(result[\"text\"])\n",
    "    doc.save(output_file_path)\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"Transcription saved to {output_file_path}\")\n",
    "\n",
    "    return docx_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio extraction complete. The file has been saved to: audio\\A7_Continual_Improvement_Video_Tast_Cakir_Bellmann.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\famaya\\Desktop\\quickstart\\venv\\lib\\site-packages\\whisper\\transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to transcripts\\A7_Continual_Improvement_Video_Tast_Cakir_Bellmann.docx\n"
     ]
    }
   ],
   "source": [
    "# generate audio trascription from an input video\n",
    "#define video path\n",
    "video_path = r\"Videos/A7_Continual_Improvement_Video_Tast_Cakir_Bellmann.mp4\"\n",
    "#define output path to save audio\n",
    "output_path = r\"audio\"\n",
    "audio_path = extract_audio(video_path, output_path)\n",
    "#define output path to save transcripts\n",
    "transcripts_folder = \"transcripts\"\n",
    "transcript_filename = transcribe_audio(audio_path, transcripts_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Content Analysis using a RAG System with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from models/em_german_leo_mistral.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = jphme_em_german_leo_mistral\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = jphme_em_german_leo_mistral\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   488.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  488.00 MiB, K (f16):  244.00 MiB, V (f16):  244.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   283.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'jphme_em_german_leo_mistral', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =       1.42 ms /     7 runs   (    0.20 ms per token,  4929.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =  140760.93 ms /  2063 tokens (   68.23 ms per token,    14.66 tokens per second)\n",
      "llama_print_timings:        eval time =     916.88 ms /     6 runs   (  152.81 ms per token,     6.54 tokens per second)\n",
      "llama_print_timings:       total time =  141689.68 ms /  2069 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      27.44 ms /   128 runs   (    0.21 ms per token,  4664.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2934.82 ms /    34 tokens (   86.32 ms per token,    11.59 tokens per second)\n",
      "llama_print_timings:        eval time =   20116.38 ms /   127 runs   (  158.40 ms per token,     6.31 tokens per second)\n",
      "llama_print_timings:       total time =   23251.51 ms /   161 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      78.77 ms /   380 runs   (    0.21 ms per token,  4824.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3521.35 ms /    45 tokens (   78.25 ms per token,    12.78 tokens per second)\n",
      "llama_print_timings:        eval time =   57901.01 ms /   379 runs   (  152.77 ms per token,     6.55 tokens per second)\n",
      "llama_print_timings:       total time =   62107.57 ms /   424 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =       8.30 ms /    41 runs   (    0.20 ms per token,  4941.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2042.86 ms /    25 tokens (   81.71 ms per token,    12.24 tokens per second)\n",
      "llama_print_timings:        eval time =    5888.75 ms /    40 runs   (  147.22 ms per token,     6.79 tokens per second)\n",
      "llama_print_timings:       total time =    7989.86 ms /    65 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      16.72 ms /    76 runs   (    0.22 ms per token,  4545.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   35522.82 ms /   523 tokens (   67.92 ms per token,    14.72 tokens per second)\n",
      "llama_print_timings:        eval time =   11262.77 ms /    75 runs   (  150.17 ms per token,     6.66 tokens per second)\n",
      "llama_print_timings:       total time =   46899.51 ms /   598 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =       8.05 ms /    39 runs   (    0.21 ms per token,  4844.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   69565.89 ms /  1014 tokens (   68.61 ms per token,    14.58 tokens per second)\n",
      "llama_print_timings:        eval time =    5478.85 ms /    38 runs   (  144.18 ms per token,     6.94 tokens per second)\n",
      "llama_print_timings:       total time =   75098.78 ms /  1052 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      16.48 ms /    77 runs   (    0.21 ms per token,  4672.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1767.94 ms /    22 tokens (   80.36 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:        eval time =   11335.71 ms /    76 runs   (  149.15 ms per token,     6.70 tokens per second)\n",
      "llama_print_timings:       total time =   13216.17 ms /    98 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =       1.29 ms /     6 runs   (    0.22 ms per token,  4633.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8884.54 ms /    55 tokens (  161.54 ms per token,     6.19 tokens per second)\n",
      "llama_print_timings:        eval time =     738.91 ms /     5 runs   (  147.78 ms per token,     6.77 tokens per second)\n",
      "llama_print_timings:       total time =    9632.25 ms /    60 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      33.12 ms /   154 runs   (    0.22 ms per token,  4649.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4731.11 ms /    59 tokens (   80.19 ms per token,    12.47 tokens per second)\n",
      "llama_print_timings:        eval time =   22427.83 ms /   153 runs   (  146.59 ms per token,     6.82 tokens per second)\n",
      "llama_print_timings:       total time =   27392.54 ms /   212 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      13.03 ms /    62 runs   (    0.21 ms per token,  4759.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3164.58 ms /    39 tokens (   81.14 ms per token,    12.32 tokens per second)\n",
      "llama_print_timings:        eval time =    8890.26 ms /    61 runs   (  145.74 ms per token,     6.86 tokens per second)\n",
      "llama_print_timings:       total time =   12142.22 ms /   100 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =       1.28 ms /     6 runs   (    0.21 ms per token,  4702.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3912.17 ms /    47 tokens (   83.24 ms per token,    12.01 tokens per second)\n",
      "llama_print_timings:        eval time =     721.66 ms /     5 runs   (  144.33 ms per token,     6.93 tokens per second)\n",
      "llama_print_timings:       total time =    4641.97 ms /    52 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =       9.20 ms /    44 runs   (    0.21 ms per token,  4781.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2985.33 ms /    33 tokens (   90.46 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:        eval time =    6301.44 ms /    43 runs   (  146.55 ms per token,     6.82 tokens per second)\n",
      "llama_print_timings:       total time =    9349.14 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      12.86 ms /    61 runs   (    0.21 ms per token,  4744.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4241.87 ms /    55 tokens (   77.12 ms per token,    12.97 tokens per second)\n",
      "llama_print_timings:        eval time =    8730.57 ms /    60 runs   (  145.51 ms per token,     6.87 tokens per second)\n",
      "llama_print_timings:       total time =   13058.33 ms /   115 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =       1.32 ms /     6 runs   (    0.22 ms per token,  4545.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2835.50 ms /    37 tokens (   76.64 ms per token,    13.05 tokens per second)\n",
      "llama_print_timings:        eval time =     723.55 ms /     5 runs   (  144.71 ms per token,     6.91 tokens per second)\n",
      "llama_print_timings:       total time =    3567.52 ms /    42 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      25.28 ms /   119 runs   (    0.21 ms per token,  4707.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2835.85 ms /    37 tokens (   76.64 ms per token,    13.05 tokens per second)\n",
      "llama_print_timings:        eval time =   17279.01 ms /   118 runs   (  146.43 ms per token,     6.83 tokens per second)\n",
      "llama_print_timings:       total time =   20291.76 ms /   155 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      29.12 ms /   137 runs   (    0.21 ms per token,  4704.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9579.83 ms /    95 tokens (  100.84 ms per token,     9.92 tokens per second)\n",
      "llama_print_timings:        eval time =   19956.62 ms /   136 runs   (  146.74 ms per token,     6.81 tokens per second)\n",
      "llama_print_timings:       total time =   29748.52 ms /   231 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      21.98 ms /   100 runs   (    0.22 ms per token,  4548.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5561.28 ms /    58 tokens (   95.88 ms per token,    10.43 tokens per second)\n",
      "llama_print_timings:        eval time =   15080.61 ms /    99 runs   (  152.33 ms per token,     6.56 tokens per second)\n",
      "llama_print_timings:       total time =   20790.51 ms /   157 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      71.00 ms /   328 runs   (    0.22 ms per token,  4619.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =  144179.41 ms /  2048 tokens (   70.40 ms per token,    14.20 tokens per second)\n",
      "llama_print_timings:        eval time =   49790.12 ms /   327 runs   (  152.26 ms per token,     6.57 tokens per second)\n",
      "llama_print_timings:       total time =  194550.81 ms /  2375 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      24.55 ms /   119 runs   (    0.21 ms per token,  4847.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =  133957.51 ms /  2040 tokens (   65.67 ms per token,    15.23 tokens per second)\n",
      "llama_print_timings:        eval time =   17500.69 ms /   118 runs   (  148.31 ms per token,     6.74 tokens per second)\n",
      "llama_print_timings:       total time =  151637.48 ms /  2158 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      22.01 ms /   103 runs   (    0.21 ms per token,  4680.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2016.74 ms /    26 tokens (   77.57 ms per token,    12.89 tokens per second)\n",
      "llama_print_timings:        eval time =   16476.31 ms /   102 runs   (  161.53 ms per token,     6.19 tokens per second)\n",
      "llama_print_timings:       total time =   18645.51 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      32.51 ms /   158 runs   (    0.21 ms per token,  4859.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11565.63 ms /    66 tokens (  175.24 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:        eval time =   23459.95 ms /   157 runs   (  149.43 ms per token,     6.69 tokens per second)\n",
      "llama_print_timings:       total time =   35267.81 ms /   223 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      80.71 ms /   383 runs   (    0.21 ms per token,  4745.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2125.74 ms /    21 tokens (  101.23 ms per token,     9.88 tokens per second)\n",
      "llama_print_timings:        eval time =   58878.42 ms /   382 runs   (  154.13 ms per token,     6.49 tokens per second)\n",
      "llama_print_timings:       total time =   61703.94 ms /   403 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      35.70 ms /   165 runs   (    0.22 ms per token,  4622.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3688.50 ms /    48 tokens (   76.84 ms per token,    13.01 tokens per second)\n",
      "llama_print_timings:        eval time =   27061.69 ms /   164 runs   (  165.01 ms per token,     6.06 tokens per second)\n",
      "llama_print_timings:       total time =   31014.62 ms /   212 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      13.59 ms /    63 runs   (    0.22 ms per token,  4634.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4755.29 ms /    45 tokens (  105.67 ms per token,     9.46 tokens per second)\n",
      "llama_print_timings:        eval time =    9327.10 ms /    62 runs   (  150.44 ms per token,     6.65 tokens per second)\n",
      "llama_print_timings:       total time =   14171.80 ms /   107 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =       5.11 ms /    25 runs   (    0.20 ms per token,  4895.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2497.89 ms /    29 tokens (   86.13 ms per token,    11.61 tokens per second)\n",
      "llama_print_timings:        eval time =    3564.63 ms /    24 runs   (  148.53 ms per token,     6.73 tokens per second)\n",
      "llama_print_timings:       total time =    6096.21 ms /    53 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      29.80 ms /   132 runs   (    0.23 ms per token,  4429.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3774.66 ms /    49 tokens (   77.03 ms per token,    12.98 tokens per second)\n",
      "llama_print_timings:        eval time =   21661.70 ms /   131 runs   (  165.36 ms per token,     6.05 tokens per second)\n",
      "llama_print_timings:       total time =   25654.35 ms /   180 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      22.76 ms /   104 runs   (    0.22 ms per token,  4570.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3595.14 ms /    46 tokens (   78.16 ms per token,    12.80 tokens per second)\n",
      "llama_print_timings:        eval time =   15642.81 ms /   103 runs   (  151.87 ms per token,     6.58 tokens per second)\n",
      "llama_print_timings:       total time =   19392.84 ms /   149 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      31.97 ms /   148 runs   (    0.22 ms per token,  4629.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   35913.63 ms /   536 tokens (   67.00 ms per token,    14.92 tokens per second)\n",
      "llama_print_timings:        eval time =   21417.02 ms /   147 runs   (  145.69 ms per token,     6.86 tokens per second)\n",
      "llama_print_timings:       total time =   57553.76 ms /   683 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      22.93 ms /   108 runs   (    0.21 ms per token,  4710.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   74414.25 ms /  1037 tokens (   71.76 ms per token,    13.94 tokens per second)\n",
      "llama_print_timings:        eval time =   16200.00 ms /   107 runs   (  151.40 ms per token,     6.60 tokens per second)\n",
      "llama_print_timings:       total time =   90766.67 ms /  1144 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =       7.79 ms /    36 runs   (    0.22 ms per token,  4624.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3506.19 ms /    43 tokens (   81.54 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:        eval time =    5514.52 ms /    35 runs   (  157.56 ms per token,     6.35 tokens per second)\n",
      "llama_print_timings:       total time =    9073.31 ms /    78 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   34415.90 ms\n",
      "llama_print_timings:      sample time =      18.31 ms /    86 runs   (    0.21 ms per token,  4696.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3956.72 ms /    43 tokens (   92.02 ms per token,    10.87 tokens per second)\n",
      "llama_print_timings:        eval time =   12758.10 ms /    85 runs   (  150.10 ms per token,     6.66 tokens per second)\n",
      "llama_print_timings:       total time =   16832.43 ms /   128 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedback generation is done\n"
     ]
    }
   ],
   "source": [
    "# convert prompt format for german_leo_mistral model\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"Du bist ein hilfreicher Assistent. USER: {message.content} ASSISTANT.\"  \n",
    "    return prompt\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "    return f\" ASSISTANT: {completion}\"\n",
    "\n",
    "#download the model from Huggingface to the folder .\\models using this link https://huggingface.co/TheBloke/em_german_leo_mistral-GGUF/blob/main/em_german_leo_mistral.Q4_K_M.gguf\n",
    "model_path = \"models/em_german_leo_mistral.Q4_K_M.gguf\"\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# initialze the llm\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    model_url=None,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=model_path,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=1000,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    #model_kwargs={\"n_gpu_layers\": 1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    #completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    "\n",
    "#initialize the tokenizer\n",
    ")\n",
    "\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\").encode\n",
    ")\n",
    "\n",
    "#define the path to the video transript\n",
    "\n",
    "# load documents\n",
    "document_path = os.path.join(\"transcripts\", transcript_filename)\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[document_path]\n",
    ").load_data()\n",
    "\n",
    "# create vector store index\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "# set up query engine\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "\n",
    "#read prompts json\n",
    "\n",
    "with open('prompts.json', 'r') as file:\n",
    "    prompts = json.load(file)\n",
    "\n",
    "#create a new document\n",
    "doc = Document()\n",
    "\n",
    "\n",
    "#iterate through rubrics and prompts and generate a response\n",
    "for rubric_key, prompts in prompts.items():\n",
    "    responses = {key: [] for key in prompts}\n",
    "\n",
    "    for criterion, prompt in prompts.items():\n",
    "        response = query_engine.query(prompt)\n",
    "        responses[criterion].append(response)\n",
    "    doc.add_heading(rubric_key, level=1)\n",
    "\n",
    "\n",
    "    # save the reponses in a docx file\n",
    "    for category, response_list in responses.items():\n",
    "        doc.add_heading(category, level=2)\n",
    "        for response in response_list:\n",
    "            doc.add_paragraph(str(response))\n",
    "            doc.add_paragraph() \n",
    "\n",
    "\n",
    "# Save the document\n",
    "document_path = os.path.join(\"output_feedback\", transcript_filename)\n",
    "doc.save(document_path)\n",
    "print(\"feedback generation is done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
